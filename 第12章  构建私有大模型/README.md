# åŸºäº ChatGLM3-6B æ„å»ºç§æœ‰å¤§æ¨¡å‹
## ç¯å¢ƒæ­å»º
å°† ChatGLM2-6B å¾®è°ƒä»£ç å¤åˆ¶åˆ° ChatGLM3-6B ç›®å½•
```text
cp -r ChatGLM2-6B/ptuning ChatGLM3-6B/
```
å®‰è£…ç›¸å…³ä¾èµ–ï¼š
```text
pip install -r requirements.txt
```
å…¶ä¸­ï¼Œtransformersåº“ç‰ˆæœ¬æ¨èä¸º4.30.2ï¼Œtorchæ¨èä½¿ç”¨2.0åŠä»¥ä¸Šçš„ç‰ˆæœ¬ï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨ç†æ€§èƒ½ã€‚
## ä»£ç è°ƒç”¨
```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True, device='cuda')
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ChatGLM3-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
>>> response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
>>> print(response)

æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:
	
åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚
å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
```
## ç½‘é¡µç‰ˆç¤ºä¾‹
å¯åŠ¨ä¸€ä¸ªåŸºäºGradioçš„ç½‘é¡µç‰ˆç¤ºä¾‹ï¼š
```text
python web_demo.py
```

![](../images/åŸºäºGradioå¯åŠ¨ChatGLM3-6B.png)

é™¤äº†ä¸Šé¢çš„æ–¹å¼ï¼Œè¿˜å¯ä»¥é€šè¿‡å¦‚ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨ä¸€ä¸ªåŸºäºStreamlitçš„ç½‘é¡µç‰ˆç¤ºä¾‹ã€‚
```text
streamlit run web_demo2.py
```

![](../images/åŸºäºStreamlitå¯åŠ¨ChatGLM3-6B.png)

## å‘½ä»¤è¡Œç¤ºä¾‹
è¿˜å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨ï¼š
```text
python cli_demo.py
```

![](../images/åŸºäºå‘½ä»¤è¡Œå¯åŠ¨ChatGLM3-6B.png)

## ä½æˆæœ¬éƒ¨ç½²
æ¨¡å‹é»˜è®¤ä»¥FP16ç²¾åº¦åŠ è½½ï¼Œéœ€è¦çº¦13â€…GBçš„æ˜¾å­˜ã€‚å¦‚æœæ‚¨çš„GPUæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥é€‰æ‹©ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œå…·ä½“æ–¹æ³•å¦‚ä¸‹ï¼š
```python
model = AutoModel.from_pretrained("THUDM/chatglm3-6b",trust_remote_code=True).quantize(4)
```
ä¹Ÿå¯ä»¥åœ¨CPUä¸Šè¿è¡Œæ¨¡å‹ï¼Œä½†æ˜¯æ¨ç†é€Ÿåº¦ä¼šæ…¢å¾ˆå¤šã€‚å…·ä½“æ–¹æ³•å¦‚ä¸‹ï¼ˆéœ€è¦è‡³å°‘32â€…GBçš„å†…å­˜ï¼‰ï¼š
```python
model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).float()
```
å¦‚æœæ‚¨çš„Macä½¿ç”¨äº†Apple Siliconæˆ–AMD GPUï¼Œå¯ä»¥ä½¿ç”¨MPSåç«¯åœ¨GPUä¸Šè¿è¡ŒChatGLM3-6Bã€‚
```python
model = AutoModel.from_pretrained("your local path", trust_remote_code=True).to('mps')
```
å¦‚æœæ‚¨æœ‰å¤šå¼ GPUï¼Œä½†æ˜¯æ¯å¼ GPUçš„æ˜¾å­˜éƒ½ä¸å¤ŸåŠ è½½å®Œæ•´çš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹å¹¶è¡Œçš„æ–¹å¼ï¼Œå°†æ¨¡å‹åˆ†é…åˆ°å¤šå¼ GPUä¸Šã€‚
```python
from utils import load_model_on_gpus
model = load_model_on_gpus("THUDM/chatglm3-6b", num_gpus=2)
```

## å¾®è°ƒ ChatGLM3-6B
### æ•°æ®å‡†å¤‡
ä¸‹è½½ADGENæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¹¿å‘Šæ–‡æ¡ˆçš„æ•°æ®é›†ã€‚ADGENæ•°æ®é›†çš„ä»»åŠ¡æ˜¯æ ¹æ®è¾“å…¥çš„å•†å“ä¿¡æ¯ç”Ÿæˆä¸€æ®µå¸å¼•äººçš„å¹¿å‘Šè¯ã€‚æŠŠAdvertiseGenæ–‡ä»¶å¤¹é‡Œçš„æ•°æ®åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œåˆ†åˆ«ä¿å­˜ä¸ºtrain.jsonå’Œdev.jsonæ–‡ä»¶ï¼Œæ•°æ®çš„æ ¼å¼å¦‚ä¸‹ï¼š

![](../images/ADGENæ•°æ®é›†çš„æ•°æ®æ ¼å¼.png)

### ç¯å¢ƒå®‰è£…
è¦è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œæ‚¨éœ€è¦å…ˆå®‰è£…deepspeedï¼Œè¿˜éœ€è¦å®‰è£…ä¸€äº›æœ‰ç›‘ç£å¾®è°ƒéœ€è¦çš„åŒ…ã€‚
```text
pip install deepspeed
cd ptuning
pip install rouge_chinese nltk jieba datasets
```
ä¿®æ”¹å¾®è°ƒä»£ç ä¸­çš„ç›¸å…³å‚æ•°
```text
vim ds_train_finetune.sh
LR=1e-5
MASTER_PORT=$(shuf -n 1 -i 10000-65535)
deepspeed --num_gpus=8 --master_port $MASTER_PORT main.py \
    --deepspeed deepspeed.json \
    --do_train \
    --preprocessing_num_workers 32 \
    --train_file AdvertiseGen/train.json \
    --test_file AdvertiseGen/dev.json \
    --prompt_column content \
    --response_column summary \
    --model_name_or_path ../models/chatglm3-6b \
    --output_dir output/adgen-chatglm3-6b-ft \
    --overwrite_output_dir \
    --max_source_length 512 \
    --max_target_length 512 \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --predict_with_generate \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate $LR \
    --fp16
```
å„ä¸ªå‚æ•°çš„å«ä¹‰å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š
| å‚æ•° | å‚æ•°å«ä¹‰ |
| --- | --- |
| num_gpus | ä½¿ç”¨çš„GPUæ•°é‡ |
| deepspeed | deepspeedçš„é…ç½®æ–‡ä»¶ |
| preprocessing_num_workers | æ•°æ®é¢„å¤„ç†çš„çº¿ç¨‹æ•°é‡ |
| train_file | è®­ç»ƒæ•°æ®æ–‡ä»¶çš„è·¯å¾„ |
| test_file | æµ‹è¯•æ•°æ®æ–‡ä»¶çš„è·¯å¾„ |
| prompt_column | è¾“å…¥åˆ—çš„åç§° |
| response_column | è¾“å‡ºåˆ—çš„åç§° |
| model_name_or_path | æ¨¡å‹åç§°æˆ–è·¯å¾„ |
| output_dir | è¾“å‡ºæ¨¡å‹å‚æ•°çš„æ–‡ä»¶è·¯å¾„ |
| max_source_length | æœ€å¤§è¾“å…¥é•¿åº¦ |
| max_target_length | æœ€å¤§è¾“å‡ºé•¿åº¦ |
| per_device_train_batch_size | æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| per_device_eval_batch_size | æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹æ¬¡å¤§å° |
| gradient_accumulation_steps | æ¢¯åº¦ç´¯è®¡æ­¥æ•° |
| predict_with_generate | æ˜¯å¦ä½¿ç”¨ç”Ÿæˆæ¨¡å¼è¿›è¡Œé¢„æµ‹ |
| logging_steps | è®°å½•æ—¥å¿—çš„æ­¥æ•° |
| save_steps | ä¿å­˜æ¨¡å‹çš„æ­¥æ•° |
| learning_rate | å­¦ä¹ ç‡ |
| fp16 | æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°è¿›è¡Œè®­ç»ƒ |

æ‰§è¡Œä¸‹é¢å‘½ä»¤ï¼Œè®­ç»ƒæ¨¡å‹ï¼š
```text
bash ds_train_finetune.sh
```
å½“è¿è¡Œä»£ç æ—¶ï¼Œä¼šé‡åˆ°ä¸€ä¸ªé”™è¯¯æç¤ºï¼šChatGLMTokenizerç±»æ²¡æœ‰build_promptæ–¹æ³•ã€‚è¿™æ˜¯å› ä¸ºChatGLM3-6Bçš„ChatGLMTokenizerç±»æ²¡æœ‰å®ç°è¿™ä¸ªæ–¹æ³•ã€‚è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥å‚è€ƒChatGLM2-6Bä¸­ChatGLMTokenizerç±»çš„build_promptæ–¹æ³•ï¼ŒæŒ‰ç…§ç›¸åŒçš„é€»è¾‘ç¼–å†™ä»£ç ã€‚
```python
vim ../models/chatglm3-6b/tokenization_chatglm.py
# åœ¨ChatGLMTokenizerç±»ä¸­å®ç°build_promptæ–¹æ³•
def build_prompt(self, query, history=None):
    if history is None:
	history = []
	prompt = ""
	for i, (old_query, response) in enumerate(history):
	    prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š{}\n\n".format(
		    i + 1, old_query, response)
	prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š".format(len(history) + 1, query)
    return prompt
```
### æ¨¡å‹å¾®è°ƒ
åœ¨Tokenizerç±»ä¸­æ·»åŠ äº†build_promptæ–¹æ³•çš„ä»£ç åï¼Œç»§ç»­æ‰§è¡Œbash ds_train_finetune.shå‘½ä»¤ã€‚ç¨‹åºå°±å¯ä»¥æ­£å¸¸è¿è¡Œäº†ã€‚
æ¨¡å‹å¾®è°ƒå®Œæˆåï¼Œ./output/adgen-chatglm3-6b-ftç›®å½•ä¸‹ä¼šç”Ÿæˆç›¸åº”çš„æ–‡ä»¶ï¼ŒåŒ…å«æ¨¡å‹çš„å‚æ•°æ–‡ä»¶å’Œå„ç§é…ç½®æ–‡ä»¶ã€‚
```text
tree ./output/adgen-chatglm3-6b-ft
â”œâ”€â”€ all_results.json
â”œâ”€â”€ checkpoint-1000
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ configuration_chatglm.py
â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”œâ”€â”€ global_step1000
â”‚   â”‚   â”œâ”€â”€ mp_rank_00_model_states.pt
â”‚   â”‚   â”œâ”€â”€ zero_pp_rank_0_mp_rank_00_optim_states.pt
â”‚   â”‚   â”œâ”€â”€ zero_pp_rank_1_mp_rank_00_optim_states.pt
â”‚   â”‚   â”œâ”€â”€ zero_pp_rank_2_mp_rank_00_optim_states.pt
â”‚   â”‚   â”œâ”€â”€ zero_pp_rank_3_mp_rank_00_optim_states.pt
â”‚   â”‚   â”œâ”€â”€ zero_pp_rank_4_mp_rank_00_optim_states.pt
â”‚   â”‚   â”œâ”€â”€ zero_pp_rank_5_mp_rank_00_optim_states.pt
â”‚   â”‚   â”œâ”€â”€ zero_pp_rank_6_mp_rank_00_optim_states.pt
â”‚   â”‚   â””â”€â”€ zero_pp_rank_7_mp_rank_00_optim_states.pt
â”‚   â”œâ”€â”€ ice_text.model
â”‚   â”œâ”€â”€ latest
â”‚   â”œâ”€â”€ modeling_chatglm.py
â”‚   â”œâ”€â”€ pytorch_model-00001-of-00002.bin
â”‚   â”œâ”€â”€ pytorch_model-00002-of-00002.bin
â”‚   â”œâ”€â”€ pytorch_model.bin.index.json
â”‚   â”œâ”€â”€ quantization.py
â”‚   â”œâ”€â”€ rng_state_0.pth
â”‚   â”œâ”€â”€ rng_state_1.pth
â”‚   â”œâ”€â”€ rng_state_2.pth
â”‚   â”œâ”€â”€ rng_state_3.pth
â”‚   â”œâ”€â”€ rng_state_4.pth
â”‚   â”œâ”€â”€ rng_state_5.pth
â”‚   â”œâ”€â”€ rng_state_6.pth
â”‚   â”œâ”€â”€ rng_state_7.pth
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenization_chatglm.py
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â”œâ”€â”€ training_args.bin
â”‚   â””â”€â”€ zero_to_fp32.py
â”œâ”€â”€ trainer_state.json
â””â”€â”€ train_results.json
```
### æ¨¡å‹éƒ¨ç½²
æ‰§è¡Œstreamlit run web_demo2.pyå‘½ä»¤æ¥å¯åŠ¨æ–°æ¨¡å‹ã€‚

![](../images/å¾®è°ƒä¹‹åçš„æ¨¡å‹.png)

